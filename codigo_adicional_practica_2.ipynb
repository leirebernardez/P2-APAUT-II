{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dd523c88",
   "metadata": {},
   "source": [
    "# Examen práctica 2\n",
    "## Aprendizaje Automático II\n",
    "\n",
    "---\n",
    "\n",
    "## Importaciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8b42970",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.tree import DecisionTreeRegressor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f09dd33f",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Examen sorpresa resuelto\n",
    "\n",
    "### 1. Lectura y guardado de datos\n",
    "Lee los datos de df1 desde el archivo datos_1.csv. Guarda el DataFrame df2 en un csv datos_2.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37f0fbec",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv(\"datos_1.csv\")\n",
    "df2 = pd.to_csv(\"datos_2.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5a18385",
   "metadata": {},
   "source": [
    "### 2. Limpieza de valores faltantes\n",
    "Elimina todas las filas con NaN en ambos DataFrame. Puedes hacerlo de forma segura sin perder el índice o documenta tu decisión."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eff1b2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = df1.dropna()\n",
    "df2 = df2.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2c61f4d",
   "metadata": {},
   "source": [
    "### 3. Eliminación de columna innecesaria\n",
    "En df2 elimina la columna inutil_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc82826b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = df2.drop(columns=[\"inutil_1\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6445b52a",
   "metadata": {},
   "source": [
    "### 4. Construcción de df3\n",
    "Crea df3 que contenga todas las filas de df2 y, para cada id, ñada la información correspondiente de df1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f90cf86",
   "metadata": {},
   "outputs": [],
   "source": [
    "df3 = df2.merge(df1, on=\"id\", how=\"left\") # Usamos la columna de df1 de id para hacer un join"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9091f87",
   "metadata": {},
   "source": [
    "### 5. Agregación por moda\n",
    "En df3, para cada id calcula la moda de cat_3 y añádela como una nueva columna \"new_1\". Hazlo en una línea y, si hay multimoda, especifica un criterio simple (p.ej., escoger la primera)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d8cd5f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df3[\"new_1\"] = df3.groupby(\"id\")[\"cat_3\"].transform(lambda x: x.mode().loc[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f23ee30",
   "metadata": {},
   "source": [
    "### 6. Operaciones de unión y concatenación\n",
    "Añade una fila nueva a df2 con valores ficticios para cada columna."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18c2889d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.loc[len(df2)] = {\"id\":999, \"cat_3\":\"nuevo\", \"cat_4\":\"x\", \"num\":2}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "193ae2a6",
   "metadata": {},
   "source": [
    "### 7. Separación de variables\n",
    "Con df3, separa X (características) de y (target). Debe ser un vector/serie con la columna target; X no debe incluir target."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a353798",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df3[\"target\"]\n",
    "X = df3.drop(columns=[\"target\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74220304",
   "metadata": {},
   "source": [
    "### 8. Pipeline de modelado\n",
    "Crea un Pipeline que:\n",
    "- Aplique un preprocesado que transforme las variables categóricas a one-hot encoding usando sklearn.\n",
    "- Use un árbol de decisión (DecisionTreeClassifier) para predecir target.\n",
    "\n",
    "Incluye una detección programática de columnas categóricas vs columnas numéricas, división train/test, ajuste del pipeline y una métrica simple."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5695056",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importaciones\n",
    "from sklearn.base import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "# A partir de la X e y hechas en el ejercicio anterior\n",
    "\n",
    "# Detección de columnas categóricas vs columas numéricas\n",
    "cat_cols = X.select_dtypes(include=[\"object\", \"category\"]).columns\n",
    "num_cols = X.select_dtypes(include=[\"number\"]).columns\n",
    "\n",
    "# Preprocesado: [(nombre del bloque, transformador), columnas que recibe, dejar las numéricas tal cual]\n",
    "preprocesado = ColumnTransformer(transformers=[(\"cat\", OneHotEncoder(handle_unknown=\"ignore\"), cat_cols)], remainder=\"passthrough\")\n",
    "\n",
    "# Árbol de decisión\n",
    "modelo = DecisionTreeClassifier()\n",
    "\n",
    "# Pipeline\n",
    "pipe = Pipeline(steps=[(\"preprocesado\", preprocesado), (\"modelo\", modelo)])\n",
    "\n",
    "# División en train/test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_strate=42)\n",
    "\n",
    "# Ajuste del pipeline\n",
    "pipe.fit(X_train, y_train)\n",
    "\n",
    "# Métricas simples\n",
    "y_pred = pipe.predict(X_test)\n",
    "acc = accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "084d7878",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Códigos examen teórico\n",
    "\n",
    "### 1. Bagging\n",
    "Combina varios modelos entrenados en subconjuntos aleatorios del dataset (por bootstrap) para reducir la varianza.\n",
    "1. Genera B subconjuntos del dataset usando bootstrap (muestras aleatorias con reemplazo).\n",
    "2. Entrena un modelo base (por ejemplo, un árbol) en cada subconjunto.\n",
    "\n",
    "Para predecir:\n",
    "- Regresión: promedio de las predicciones.\n",
    "- Clasificación: voto mayoritario."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c62d33e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Bagging:\n",
    "    def __init__(self, max_estimators, max_depth):\n",
    "        self.max_estimators = max_estimators\n",
    "        self.max_depth = max_depth\n",
    "        self.estimators = []\n",
    "        \n",
    "    def fit(self, x, t):\n",
    "        ix = np.random.randint(0, x.shape[0], x.shape[0])\n",
    "        x_samples = x[ix]\n",
    "        t_samples = t[ix]\n",
    "        clf = DecisionTreeRegressor(criterion='squared_error', max_depth=self.max_depth)\n",
    "        clf.fit(x_samples, t_samples)\n",
    "        self.estimators.append(clf)\n",
    "        \n",
    "    def predict(self, x):\n",
    "        y = np.zeros(x.shape)\n",
    "        for i in range(self.estimators):\n",
    "            y += self.estimators[i].predict(x)\n",
    "        return y / self.estimators"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "634c7b1e",
   "metadata": {},
   "source": [
    "### 2. Random Forest\n",
    "Es un bagging de árboles donde además se elige aleatoriamente un subconjunto de características en cada división, reduciendo la correlación entre árboles.\n",
    "1. Igual que Bagging: usa bootstrap para crear subconjuntos.\n",
    "2. En cada división del árbol, elige un subconjunto aleatorio de variables.\n",
    "3. Entrena muchos árboles con esas restricciones.\n",
    "4. Promedia o vota las predicciones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b032ff02",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomForest:\n",
    "    def __init__(self, num_estimators, max_depth):\n",
    "        self.num_estimators = num_estimators\n",
    "        self.max_depth = max_depth\n",
    "        self.estimators = []\n",
    "        \n",
    "    def fit(self, x, t):\n",
    "        n_samples, n_features = x.shape\n",
    "        max_features = int(np.sqrt(n_features))\n",
    "        \n",
    "        for _ in range(self.num_estimators):\n",
    "            ix = np.random.randint(0, n_samples, n_samples)\n",
    "            x_samples = x[ix]\n",
    "            t_samples = t[ix]\n",
    "            \n",
    "            feature_idx = np.random.choice(n_features, max_features, replace=False)\n",
    "            \n",
    "            clf = DecisionTreeRegressor(criterion='squared_error', max_depth=self.max_depth)\n",
    "            clf.fit(x_samples[:, feature_idx], t_samples)\n",
    "            clf.feature_inx = feature_idx\n",
    "            self.estimators.append(clf)\n",
    "            \n",
    "    def predict(self, x):\n",
    "        y = np.zeros(x.shape)\n",
    "        for i in range(self.num_estimators):\n",
    "            feature_idx = self.estimators[i].feature_idx\n",
    "            y += self.estimators[i].predict(x[:, feature_idx])\n",
    "        return y / self.estimators"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2714a11f",
   "metadata": {},
   "source": [
    "### 3. Boosting\n",
    "Entrena modelos secuencialmente, donde cada uno corrige los errores del anterior, reduciendo sesgo y varianza.\n",
    "1. Inicializa el modelo con una predicción básica (por ejemplo, la media de y).\n",
    "2. Calcula los errores del modelo.\n",
    "3. Entrena un nuevo modelo que corrija esos errores (dando más peso a los ejemplos mal predichos).\n",
    "4. Combina el nuevo modelo con los anteriores (por suma ponderada).\n",
    "5. Repite varias iteraciones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "960f5a32",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Boosting:\n",
    "   def __init__(self, num_estimators, max_depth):\n",
    "    self.num_estimators = num_estimators\n",
    "    self.max_depth = max_depth\n",
    "    self.estimators = []\n",
    "\n",
    "   def fit(self, x, t):\n",
    "    r = t\n",
    "    for i in range(self.num_estimators):\n",
    "      # Estimator:\n",
    "      clf = DecisionTreeRegressor(criterion='squared_error', max_depth=self.max_depth)\n",
    "      clf.fit(x, r)\n",
    "      self.estimators.append(clf)\n",
    "\n",
    "      # Update residual:\n",
    "      r = r - clf.predict(x)[:, None]\n",
    "\n",
    "   def predict(self, x):\n",
    "    y = np.zeros(x.shape[0])\n",
    "    for i in range(self.num_estimators):\n",
    "      y += self.estimators[i].predict(x)\n",
    "    return y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "684df7c1",
   "metadata": {},
   "source": [
    "### 4. Gradient Boosting\n",
    "Versión del boosting que ajusta cada nuevo modelo al gradiente del error (residuos), logrando gran precisión.\n",
    "1. Empieza con una predicción inicial (p. ej. media de y).\n",
    "2. Calcula los residuos (gradiente negativo del error).\n",
    "3. Ajusta un modelo débil (p. ej. árbol pequeño) a esos residuos.\n",
    "4. Actualiza la predicción con η learning rate.\n",
    "5. Repite hasta converger o alcanzar N iteraciones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e0411ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GradientBoosting:\n",
    "   def __init__(self, num_estimators, max_depth, learning_rate, gradient_function):\n",
    "    self.num_estimators = num_estimators\n",
    "    self.max_depth = max_depth\n",
    "    self.learning_rate = learning_rate\n",
    "    self.gradient_function = gradient_function\n",
    "    self.estimators = []\n",
    "\n",
    "   def fit(self, x, t):\n",
    "    # Initial estimator:\n",
    "    clf = DecisionTreeRegressor(criterion='squared_error', max_depth=self.max_depth)\n",
    "    clf.fit(x, t)\n",
    "    self.estimators.append(clf)\n",
    "\n",
    "    # Gradient residual:\n",
    "    F = clf.predict(x)[:, None]\n",
    "    r = -self.gradient_function(F, t)\n",
    "\n",
    "    for i in range(1, self.num_estimators):\n",
    "      # Estimator:\n",
    "      clf = DecisionTreeRegressor(criterion='squared_error', max_depth=self.max_depth)\n",
    "      clf.fit(x, r)\n",
    "      self.estimators.append(clf)\n",
    "\n",
    "      # Update residual:\n",
    "      F = F + self.learning_rate*clf.predict(x)[:, None]\n",
    "      r = -self.gradient_function(F, t)\n",
    "\n",
    "   def predict(self, x):\n",
    "    y = self.estimators[0].predict(x)\n",
    "    for i in range(1, self.num_estimators):\n",
    "      y += self.learning_rate*self.estimators[i].predict(x)\n",
    "    return y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32177b12",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Apuntes teoría\n",
    "\n",
    "### 1. ¿Qué métricas hay que usar con cada tipo de problema?\n",
    "\n",
    "- En problemas de regresión: MSE, MAE, RMSE, R2.\n",
    "- En problemas de clasificación: Accuracy, Precision, Recall, F1-score.\n",
    "\n",
    "### 2. ¿Cuándo se usa stratify?\n",
    "\n",
    "- En problemas de clasificación porque la variable objetivo es discreta (son clases).\n",
    "- Cuando la variable objetivo está muy desbalanceada (por ejemplo 90% clase A y 10% clase B).\n",
    "\n",
    "### 3. ¿Cuándo se tienen que escalar los datos?\n",
    "\n",
    "- En problemas de clasificación solo si usa distancias como en k-NN.\n",
    "- En problemas de regresión en la mayoría de casos, especialmente si las variables no están en la misma escala/unidad y si usamos regresión lineal/ridge/lasso.\n",
    "- Cuando los atributos numéricos tienen escalas muy diferentes y el modelo depende de distancias o gradientes.\n",
    "- No es necesario escalar en Árboles de decisión, Random Forest o Gradient Boosting.\n",
    "- Por ejemplo: StandardScaler o MinMaxScaler.\n",
    "\n",
    "### 4. ¿Que hace el random_state de train_test_split?\n",
    "\n",
    "- Fija la semilla del generador aleatorio usado por la función para dividir los datos en entrenamiento y prueba.\n",
    "- Permite reproducibilidad en los experimentos."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d835007e",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Códigos de la práctica interesantes\n",
    "\n",
    "### 1. Descargar datos de sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4df6bffb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_california_housing\n",
    "data = fetch_california_housing()\n",
    "X, y = data.data, data.target"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "525d1ddd",
   "metadata": {},
   "source": [
    "\n",
    "### 2. Hacer un selector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4d4e37f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectFromModel\n",
    "\n",
    "selector = SelectFromModel(decision_tree, threshold=0.01, prefit=True) # type: ignore\n",
    "X_selected = selector.transform(X)\n",
    "\n",
    "print(f\"Número de variables originales: {X.shape[1]}\")\n",
    "print(f\"Número de variables seleccionadas: {X_selected.shape[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c02b06a",
   "metadata": {},
   "source": [
    "### 3. Preprocesado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16456538",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imprimir el total de valores nulos\n",
    "from regression_tree import RegressionTree\n",
    "\n",
    "\n",
    "df = pd.DataFrame(X, columns=data.feature_names)\n",
    "print(df.isnull().sum())      \n",
    "\n",
    "# Ver generalmente el dataset\n",
    "print(f\"Shape X: {X.shape}\")\n",
    "print(f\"Shape y: {y.shape}\")\n",
    "print(df.describe())\n",
    "\n",
    "# Entrenamos el árbol óptimo encontrado en el apartado 1\n",
    "tree = RegressionTree(max_depth=best_depth, min_samples_split=5) # type: ignore\n",
    "tree.fit(X_train, y_train)\n",
    "\n",
    "# Seleccionamos dos muestras del conjunto de test\n",
    "x1 = X_test[0]\n",
    "x2 = X_test[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "959421c0",
   "metadata": {},
   "source": [
    "### 4. Pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e066363c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.discriminant_analysis import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Definir pipeline con escalado y regresión logística\n",
    "pipe = Pipeline([\n",
    "    ('scaler', StandardScaler()),          # paso 1: escalado\n",
    "    ('logreg', LogisticRegression())       # paso 2: modelo\n",
    "])\n",
    "\n",
    "# Entrenar pipeline\n",
    "pipe.fit(X_train, y_train)\n",
    "\n",
    "# Predecir y evaluar\n",
    "y_pred = pipe.predict(X_test)\n",
    "acc_base = accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd3b8a13",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import BaggingClassifier\n",
    "\n",
    "# Pipeline con Bagging\n",
    "bagging_model = BaggingClassifier(\n",
    "    estimator=Pipeline([\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('logreg', LogisticRegression())\n",
    "    ]),\n",
    "    n_estimators=50,       # número de modelos base\n",
    "    max_samples=0.8,       # cada modelo ve el 80% del train (bootstrapping)\n",
    "    bootstrap=True,        # activamos bootstrapping\n",
    "    n_jobs=-1,             # usar todos los núcleos\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Entrenar pipeline\n",
    "bagging_model.fit(X_train, y_train)\n",
    "\n",
    "# Predecir y evaluar\n",
    "y_pred_bag = bagging_model.predict(X_test)\n",
    "acc_bag = accuracy_score(y_test, y_pred_bag)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
