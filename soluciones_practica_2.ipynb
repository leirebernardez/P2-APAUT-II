{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4ea4ddcb",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#f0f8ff; border-radius:8px; padding:12px; text-align:center;\">\n",
    "\n",
    "# **PRACTICA 2: Árboles y Ensembles**\n",
    "\n",
    "</div>\n",
    "\n",
    "*Aprendizaje Automático*\n",
    "\n",
    "---\n",
    "\n",
    "**Grupo:** G-7312  \n",
    "**Número de pareja:** 01  \n",
    "**Miembros:**  \n",
    "- Leire Bernárdez Vázquez  \n",
    "- Carmen Reiné Rueda\n",
    "\n",
    "---\n",
    "\n",
    "### **Importaciones**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f9dba5e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import GridSearchCV, StratifiedKFold\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.datasets import fetch_covtype\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.feature_selection import SelectFromModel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34e58b02",
   "metadata": {},
   "source": [
    "\n",
    "### **1. Clasificación mediante  ́arboles de clasificación**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd4a805c",
   "metadata": {},
   "source": [
    "---\n",
    "**1. Cargue los datos del problema**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "343b57a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Covertype dataset\n",
    "cov_type = fetch_covtype()\n",
    "# Separate the features and the target variable\n",
    "X = cov_type.data\n",
    "y = cov_type.target"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdd92d60",
   "metadata": {},
   "source": [
    "**2. Separe los datos en training (50 %) y test (50 %)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bc7d1c1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.5, random_state=42, stratify=y\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d3411fd",
   "metadata": {},
   "source": [
    "No hace falta escalarlo, lo dice la teacher (los árboles se tragan todo lo que les des)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8401128f",
   "metadata": {},
   "source": [
    "**3. Utilice la clase DecisionTreeClassifier de Sklearn para resolver el problema\n",
    "y dé un resultado de test usando la m ́etrica que considere oportuna**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "485a28a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 92.37055344812156 %\n"
     ]
    }
   ],
   "source": [
    "decision_tree = DecisionTreeClassifier()\n",
    "decision_tree.fit(X_train, y_train)\n",
    "y_pred = decision_tree.predict(X_test)\n",
    "porcentaje = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", porcentaje*100, \"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a02190d",
   "metadata": {},
   "source": [
    "**4. ¿Había missing values en nuestro problema? ¿Qué efecto tendría en los árboles no completar los missing values ?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "29705ab3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.isnan(X_train).sum()==0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b52ffbee",
   "metadata": {},
   "source": [
    "Si existieran y no se imputaran, DecisionTreeClassifier daría error al entrenar, porque los árboles no soportan missing values; requieren que todos los datos estén completos para funcionar. Para solucionar este problema, se podrían imputar con media, mediana o un valor concreto."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7f58d6a",
   "metadata": {},
   "source": [
    "**5. ¿Con qué parémetro podría controlarse la profundidad del ́arbol? ¿Qué ocurre si un ́arbol se inicializa sin dar un valor concreto a ese parámetro? ¿Crees que sería una buena práctica no darle ningún valor?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b2256c1",
   "metadata": {},
   "source": [
    "a) La profundidad máxima del árbol se controla con el parámetro max_depth directamente, que por defecto es max_depth=None. Sin embargo, también se puede controlar indirectamente con min_samples_split, que no divide un nodo si tiene menos muestras que las requeridas, y con min_samples_leaf, que obliga a que cada hoja tenga un mínimo de muestras para evitar que existan ramas muy profundas con pocas muestras. Por lo tanto, con estos valores se reduce la profundidad máxima alcanzada.\n",
    "\n",
    "b) Crecería tanto como datos se le pasaran sin ningún tipo de límite.\n",
    "\n",
    "c) No, debido a que sin un límite establecido, los árboles grandes consumen más memoria, tardan más en entrenar y tienden a sobreajustarse más fácilmente. Con lo cual, sería importante definir un max_depth razonable teniendo en cuenta los datos."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d172a45d",
   "metadata": {},
   "source": [
    "**6. ¿Son sensibles los ́arboles al desequilibrio entre clases?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39aabafc",
   "metadata": {},
   "source": [
    "Sí, los árboles de decisión se ven afectados cuando las clases están desbalanceadas. Tratan de que cada hoja tenga solo un tipo de clase, así que cuando una clase es mayoritaria, las divisiones tienden a favorecerla. Esto puede hacer que las clases minoritarias queden mezcladas o se clasifiquen mal.\n",
    "\n",
    "Si no se corrige este desequilibrio, el árbol aprenderá principalmente la clase mayoritaria y puede fallar al predecir las clases minoritarias, aunque acierte mucho con la clase que domina.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c329e056",
   "metadata": {},
   "source": [
    "**7. ¿Existe desequilibrio entre las clases de nuestro problema? ¿Con qué parámetro de la clase DecisionTreeClassifier podrías corregirlo?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b18715e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1: 211840, 2: 283301, 3: 35754, 4: 2747, 5: 9493, 6: 17367, 7: 20510}\n"
     ]
    }
   ],
   "source": [
    "counts = np.bincount(y)  # y debe ser enteros >=0\n",
    "print({i: count for i, count in enumerate(counts) if count > 0})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e21fe9c",
   "metadata": {},
   "source": [
    "a) Para comprobar si existe desequilibrio entre las clases del dataset Covtype, hemos utilizado la función np.bincount sobre las etiquetas de cada muestra.  \n",
    "\n",
    "   Esta función nos permite contar cuántas muestras hay de cada clase y así analizar la distribución de los datos. Los resultados muestran que algunas clases tienen muchas más muestras que otras, confirmando que sí existe desequilibrio entre las clases.  \n",
    "\n",
    "b) Podríamos corregirlo utilizando el parámetro class_weight='balanced  en DecisionTreeClassifier, que ajusta automáticamente los pesos de cada clase en función de su frecuencia en el dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b23981d2",
   "metadata": {},
   "source": [
    "**8. ¿Cuál es la diferencia entre los parámetros min samples split y min samples leaf ?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b1ca953",
   "metadata": {},
   "source": [
    " **`min_samples_split`** \n",
    "\n",
    "**Definición según la documentación:** El número mínimo de muestras necesarias para dividir un nodo interno.\n",
    "\n",
    "Este parámetro decide si un nodo se puede dividir. \n",
    "- Si el grupo de datos tiene suficientes muestras según el valor indicado, el nodo se divide en dos o más partes.  \n",
    "- Si no tiene suficientes, el nodo se queda igual y se convierte en hoja.\n",
    "\n",
    "\n",
    "**`min_samples_leaf`**  \n",
    "\n",
    "**Definición según la documentación:** El número mínimo de muestras necesarias para estar en un nodo hoja. Un punto de división a cualquier profundidad solo se considerará si sale a menos muestras de entrenamiento en cada una de las muestras de entrenamiento izquierdo y ramas derechas. Esto puede tener el efecto de suavizar el modelo\n",
    "\n",
    "Este parámetro se utiliza después de dividir un nodo para comprobar si cada hoja tiene suficientes muestras.  \n",
    "- Si alguna hoja tiene menos muestras que el valor indicado, la división no se realiza.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2520151e",
   "metadata": {},
   "source": [
    "**9. ¿Para qué sirve el parémetro criterion, qué valores puede tomar y qué significa o representa cada uno de esos valores?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9165a8ad",
   "metadata": {},
   "source": [
    "El parámetro criterion sirve para decidir qué tan buena es una división en un árbol de decisión. Es decir, indica cómo medir la calidad de los splits.\n",
    "\n",
    "Los valores que puede tomar son:\n",
    "\n",
    "- **gini** → utiliza el índice de Gini, que mide la impureza de un nodo.  \n",
    "  Busca que cada hoja tenga muestras de la misma clase lo más posible.  \n",
    "\n",
    "- **entropy** → utiliza la ganancia de información de Shannon.  \n",
    "  Calcula cuánto disminuye la incertidumbre o desorden al hacer un split.  \n",
    "  \n",
    "- **log_loss** → también mide la ganancia de información, pero usando pérdida logarítmica."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65ff286d",
   "metadata": {},
   "source": [
    "**10. El ́arbol entrenado, ¿realiza splits utilizando todas las variables disponibles?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e3450d7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variables usadas con importancia: [(0, 0.3388348139715392), (1, 0.028128228830494227), (2, 0.01730986421399671), (3, 0.06030282813122783), (4, 0.04691144085841901), (5, 0.14839823625394416), (6, 0.031282488311127364), (7, 0.033597830500244244), (8, 0.024419771987702663), (9, 0.14475755736863216), (10, 0.009002493642824017), (11, 0.00303063055395945), (12, 0.013365413460372863), (13, 0.0003924558921933753), (14, 0.00033347464544915387), (15, 0.00982474670015767), (16, 0.0013719987481363033), (17, 0.011252591807065403), (18, 0.00026533767294261915), (19, 0.0005772205308382756), (22, 0.00011716541185293036), (23, 0.0025920958831334253), (24, 0.0018960413093079178), (25, 0.000902286987993942), (26, 0.003312144619340922), (27, 0.00010163405813025485), (28, 1.0886440090901793e-05), (29, 0.000758109828337623), (30, 0.0012249898754869112), (31, 8.885855297719361e-06), (32, 0.0009745604505227973), (33, 0.002999976399983297), (34, 0.00028867306992145706), (35, 0.0069447211376388), (36, 0.008629297818570515), (37, 0.003950734097398492), (38, 0.00010364362270429465), (39, 0.00011490946643540901), (40, 0.0007925043024407576), (41, 7.684309013627396e-05), (42, 0.0065382083377000634), (43, 0.0026632160071196967), (44, 0.0058733272503229605), (45, 0.012884652155325273), (46, 0.005094916941467949), (47, 0.0002598754659091818), (48, 0.0010396261441299117), (49, 5.131106273029017e-05), (50, 0.00015373163499381705), (51, 0.0019403179784030543), (52, 0.0035029743255888725), (53, 0.0008383148903176658)]\n"
     ]
    }
   ],
   "source": [
    "importances = decision_tree.feature_importances_\n",
    "used_features = [(i, imp) for i, imp in enumerate(importances) if imp > 0]\n",
    "print(\"Variables usadas con importancia:\", used_features)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c186c62",
   "metadata": {},
   "source": [
    "Cada tupla (índice, importancia) indica qué columna se utilizó en algún split del árbol y cuánto aportó a la decisión de dividir los nodos.\n",
    "\n",
    "Observando las importancias de las variables de nuestro árbol:\n",
    "\n",
    "- No todas las variables se utilizan en los splits.  \n",
    "- Solo se usan aquellas que aportan información útil para separar las clases, mientras que las demás quedan con importancia 0 y no aparecen en ningún nodo.  \n",
    "- Aunque el árbol analiza todas las variables disponibles, solo mantiene en el modelo final las que realmente ayudan a mejorar la clasificación.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e11b88d4",
   "metadata": {},
   "source": [
    "**11. ¿Se le ocurre alguna forma de crear un selector de atributos a partir de un árbol de clasificación?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d91a3ff",
   "metadata": {},
   "source": [
    "Aprovechando la importancia de cada variable feature_importances, podemos utilizar el árbol de decisión como selector de atributos.\n",
    "\n",
    "- Manteniendo únicamente las variables que realmente aportan información al modelo.  \n",
    "- Las variables con importancia 0 o muy baja pueden eliminarse, ya que no influyen en los splits.  \n",
    "- Esto permite reducir la dimensionalidad, simplificar el modelo e incluso en algunos casos mejorar la generalización.\n",
    "\n",
    "En Scikit-Learn se puede hacer fácilmente con SelectFromModel usando el árbol entrenado `decision_tree´.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e199f75b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Número de variables originales: 54\n",
      "Número de variables seleccionadas: 13\n"
     ]
    }
   ],
   "source": [
    "selector = SelectFromModel(decision_tree, threshold=0.01, prefit=True)\n",
    "X_selected = selector.transform(X)\n",
    "\n",
    "print(f\"Número de variables originales: {X.shape[1]}\")\n",
    "print(f\"Número de variables seleccionadas: {X_selected.shape[1]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96c07117",
   "metadata": {},
   "source": [
    "SelectFromModel mira decision_tree.feature_importances_ y compara cada valor con  threshold = 0,01.  \n",
    "Si la importancia es mayor o igual al threshold, se mantiene; si es menor, se descarta.  \n",
    "Al hacer X_selected = selector.transform(X) conservamos solo las columnas importantes.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d45baaa1",
   "metadata": {},
   "source": [
    "\n",
    "### **2. Regresión mediante árboles de regresión**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "777dd13c",
   "metadata": {},
   "source": [
    "#### **2.1. Obtención de los datos**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20009eb1",
   "metadata": {},
   "source": [
    "**Descargue los datos de Sklearn**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cae91509",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_california_housing\n",
    "data = fetch_california_housing()\n",
    "X, y = data.data, data.target"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5fe3dfc",
   "metadata": {},
   "source": [
    "**Separe los datos en training (50 %) test (50 %) y realice el preprocesado de los\n",
    "datos que estime conveniente teniendo en cuenta que el modelo de aprendizaje con\n",
    "el que va a tratar de resolver el problema es un  ́arbol de regresión. Finalmente\n",
    "responda a las siguientes preguntas**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b67e919c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.5, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1b4c1d2",
   "metadata": {},
   "source": [
    "En el caso del árbol de regresión, no es necesario aplicar técnicas de escalado ni normalización, ya que no son sensibles a la escala de los atributos al realizar sus particiones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5720d60a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MedInc        0\n",
      "HouseAge      0\n",
      "AveRooms      0\n",
      "AveBedrms     0\n",
      "Population    0\n",
      "AveOccup      0\n",
      "Latitude      0\n",
      "Longitude     0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "df = pd.DataFrame(X, columns=data.feature_names)\n",
    "print(df.isnull().sum())      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4a256a46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape X: (20640, 8)\n",
      "Shape y: (20640,)\n",
      "             MedInc      HouseAge      AveRooms     AveBedrms    Population  \\\n",
      "count  20640.000000  20640.000000  20640.000000  20640.000000  20640.000000   \n",
      "mean       3.870671     28.639486      5.429000      1.096675   1425.476744   \n",
      "std        1.899822     12.585558      2.474173      0.473911   1132.462122   \n",
      "min        0.499900      1.000000      0.846154      0.333333      3.000000   \n",
      "25%        2.563400     18.000000      4.440716      1.006079    787.000000   \n",
      "50%        3.534800     29.000000      5.229129      1.048780   1166.000000   \n",
      "75%        4.743250     37.000000      6.052381      1.099526   1725.000000   \n",
      "max       15.000100     52.000000    141.909091     34.066667  35682.000000   \n",
      "\n",
      "           AveOccup      Latitude     Longitude  \n",
      "count  20640.000000  20640.000000  20640.000000  \n",
      "mean       3.070655     35.631861   -119.569704  \n",
      "std       10.386050      2.135952      2.003532  \n",
      "min        0.692308     32.540000   -124.350000  \n",
      "25%        2.429741     33.930000   -121.800000  \n",
      "50%        2.818116     34.260000   -118.490000  \n",
      "75%        3.282261     37.710000   -118.010000  \n",
      "max     1243.333333     41.950000   -114.310000  \n"
     ]
    }
   ],
   "source": [
    "print(f\"Shape X: {X.shape}\")\n",
    "print(f\"Shape y: {y.shape}\")\n",
    "print(df.describe())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f79c756",
   "metadata": {},
   "source": [
    "<span style=\"color:#2196F3\">**PREGUNTAR A LAS NIÑAS QUE HAN HECHO DE PREPROCESADO**</span>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81523487",
   "metadata": {},
   "source": [
    "**1. ¿Podría trabajar un arbol de regresión con missing values?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ece2f767",
   "metadata": {},
   "source": [
    "Sí.\n",
    "\n",
    "El modelo DecisionTreeRegressor tienen soporte integrado para valores faltantes.\n",
    "Durante el entrenamiento, el árbol evalúa las divisiones usando los datos disponibles y distribuye los valores faltantes según las reglas aprendidas.\n",
    "\n",
    "Al predecir, las muestras con valores faltantes se asignan automáticamente al nodo más probable según el comportamiento observado durante el entrenamiento.\n",
    "\n",
    "Referencia oficial: [https://scikit-learn.org/stable/modules/tree.html#missing-values-support](https://scikit-learn.org/stable/modules/tree.html#missing-values-support)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55368b3f",
   "metadata": {},
   "source": [
    "**2. ¿Es sensible un ́arbol de regresión a las magnitudes de los atributos?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b60361c",
   "metadata": {},
   "source": [
    "No.\n",
    "Los árboles de regresión no son sensibles a la escala ni a las magnitudes de los atributos, ya que las divisiones se basan únicamente en comparaciones (x_j < t) y no en distancias ni relaciones de proporción entre variables.\n",
    "\n",
    "Por eso no es necesario escalar ni normalizar los datos antes de entrenar el modelo.\n",
    "Esta es una de las ventajas que destaca la documentación oficial: los árboles de decisión requieren muy poca preparación de datos en comparación con otros modelos como KNN, que sí dependen de la escala.\n",
    "\n",
    "Referencia oficial: [https://scikit-learn.org/stable/modules/tree.html#](https://scikit-learn.org/stable/modules/tree.html#)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28ec44f9",
   "metadata": {},
   "source": [
    "**3. ¿Es sensible un árbol de regresión a la distribución de las etiquetas?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f69f63c",
   "metadata": {},
   "source": [
    "Sí, en cierta medida.\n",
    "Los árboles de regresión minimizan el error cuadrático medio (MSE) al crear las divisiones, por lo que los valores de salida muy altos o atípicos (outliers) pueden influir más en el ajuste del modelo.\n",
    "Esto significa que si las etiquetas (y) están muy desbalanceadas o contienen valores extremos, el árbol puede sobreajustarse a esos casos.\n",
    "\n",
    "Para evitarlo, la documentación recomienda controlar la complejidad del árbol con parámetros como:\n",
    "\n",
    "* max_depth → limita la profundidad máxima.\n",
    "* min_samples_split → exige un mínimo de muestras para dividir un nodo.\n",
    "* min_samples_leaf → asegura que cada hoja tenga suficientes muestras.\n",
    "\n",
    "También se puede aplicar una transformación de las etiquetas (por ejemplo, np.log1p(y)) para reducir el impacto de los valores muy grandes.\n",
    "\n",
    "Referencia oficial: [https://scikit-learn.org/stable/modules/tree.html#tips-on-practical-use](https://scikit-learn.org/stable/modules/tree.html#tips-on-practical-use)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab720984",
   "metadata": {},
   "source": [
    "#### **2.2 Implementación de un árbol de regresión**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ff440f8",
   "metadata": {},
   "source": [
    "**1. Construya una clase Nodo**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "44595126",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Nodo:\n",
    "    \"\"\"\n",
    "    Nodo del árbol de regresión.\n",
    "    Si es hoja -> value ≠ None y f_index = threshold = None\n",
    "    Si no es hoja -> define un split con f_index y threshold\n",
    "    \"\"\"\n",
    "    def __init__(self, f_index=None, threshold=None, value=None):\n",
    "        self.f_index = f_index # Indice del atributo\n",
    "        self.threshold = threshold # Umbral\n",
    "        self.value = value # Valor medio (si es hoja)\n",
    "        self.left = None # Hijo izquierdo\n",
    "        self.right = None # Hijo derecho"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe435e40",
   "metadata": {},
   "source": [
    "**2. Construya la clase RegressionTree**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54d477de",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RegressionTree:\n",
    "    def __init__(self, min_samples_split=2, max_depth=5):\n",
    "        self.min_samples_split = min_samples_split\n",
    "        self.max_depth = max_depth\n",
    "        self.raiz = None  # Nodo raíz (instancia de Nodo)\n",
    "\n",
    "    # ----------------------------\n",
    "    # 1. Cálculo del error regional\n",
    "    # ----------------------------\n",
    "    def region_error(self, y):\n",
    "        \"\"\"Devuelve la varianza de y o 0 si está vacío.\"\"\"\n",
    "        if len(y) > 0:\n",
    "            return np.var(y)\n",
    "        else:\n",
    "            return 0\n",
    "\n",
    "    # ----------------------------------\n",
    "    # 2. Búsqueda del mejor atributo y θ\n",
    "    # ----------------------------------\n",
    "        \n",
    "    def _best_split(self, X, y):\n",
    "        _, n_features = X.shape\n",
    "        best_feature, best_threshold = None, None\n",
    "        best_error = float(\"inf\")\n",
    "\n",
    "        for f in range(n_features):\n",
    "            values = np.sort(np.unique(X[:, f]))\n",
    "            if len(values) < 2:\n",
    "                continue\n",
    "            # Se decide utilizar el promedio entre valores consecutivos como umbral, en lugar de un valor exacto.\n",
    "            # Esto permite evaluar los posibles cortes de forma más estable y evita usar valores exactos del conjunto.\n",
    "            thresholds = (values[:-1] + values[1:]) / 2 \n",
    "            for t in thresholds:\n",
    "                left_idx = X[:, f] <= t\n",
    "                right_idx = ~left_idx\n",
    "                y_left, y_right = y[left_idx], y[right_idx]\n",
    "                if len(y_left) == 0 or len(y_right) == 0:\n",
    "                    continue\n",
    "                n = len(y)\n",
    "                error = (len(y_left) / n) * self._region_error(y_left) + \\\n",
    "                        (len(y_right) / n) * self._region_error(y_right)\n",
    "                if error < best_error:\n",
    "                    best_error = error\n",
    "                    best_feature = f\n",
    "                    best_threshold = t\n",
    "\n",
    "        return best_feature, best_threshold\n",
    "\n",
    "    # ----------------------------------------\n",
    "    # 3. Construcción recursiva del árbol\n",
    "    # ----------------------------------------\n",
    "    def _construir_nivel(self, X, y, profundidad):\n",
    "        # Caso base 1: profundidad agotada\n",
    "        if profundidad == 0:\n",
    "            return Nodo(value=np.mean(y))\n",
    "\n",
    "        # Caso base 2: pocas muestras\n",
    "        if len(y) <= self.min_samples_split:\n",
    "            return Nodo(value=np.mean(y))\n",
    "\n",
    "        # Buscar mejor división\n",
    "        f_opt, t_opt = self._find_best_split(X, y)\n",
    "        if f_opt is None:\n",
    "            return Nodo(value=np.mean(y))\n",
    "\n",
    "        # Crear nodo interno\n",
    "        nodo = Nodo(f_index=f_opt, threshold=t_opt)\n",
    "\n",
    "        # Dividir datos\n",
    "        left_idx = X[:, f_opt] <= t_opt\n",
    "        right_idx = ~left_idx\n",
    "\n",
    "        X_left, y_left = X[left_idx], y[left_idx]\n",
    "        X_right, y_right = X[right_idx], y[right_idx]\n",
    "\n",
    "        # Hijos\n",
    "        nodo.left = self.construir_nivel(X_left, y_left, profundidad - 1)\n",
    "        nodo.right = self.construir_nivel(X_right, y_right, profundidad - 1)\n",
    "\n",
    "        return nodo\n",
    "\n",
    "    # ----------------------------\n",
    "    # 4. Ajustar el modelo\n",
    "    # ----------------------------\n",
    "    def fit(self, X, y):\n",
    "        \"\"\"Entrena el árbol y define el nodo raíz.\"\"\"\n",
    "        self.raiz = self.construir_nivel(np.array(X), np.array(y), self.max_depth)\n",
    "\n",
    "    # ----------------------------\n",
    "    # 5. Predicción de una muestra\n",
    "    # ----------------------------\n",
    "    def _predict_sample(self, x, nodo):\n",
    "        if nodo.value is not None:\n",
    "            return nodo.value\n",
    "\n",
    "        if x[nodo.f_index] <= nodo.threshold:\n",
    "            return self._predict_sample(x, nodo.left)\n",
    "        else:\n",
    "            return self._predict_sample(x, nodo.right)\n",
    "\n",
    "    def predict(self, X):\n",
    "        X = np.array(X)\n",
    "        return np.array([self._predict_sample(x, self.raiz) for x in X])\n",
    "\n",
    "    # ----------------------------\n",
    "    # 6. Camino de decisión\n",
    "    # ----------------------------\n",
    "    def decision_path(self, x):\n",
    "        \"\"\"Devuelve un string con las decisiones seguidas hasta la predicción.\"\"\"\n",
    "        pasos = []\n",
    "        nodo = self.raiz\n",
    "        i = 1\n",
    "\n",
    "        while nodo.value is None:\n",
    "            if x[nodo.f_index] <= nodo.threshold:\n",
    "                pasos.append(f\"{i}. Atributo {nodo.f_index} menor o igual que {nodo.threshold}\")\n",
    "                nodo = nodo.left\n",
    "            else:\n",
    "                pasos.append(f\"{i}. Atributo {nodo.f_index} mayor que {nodo.threshold}\")\n",
    "                nodo = nodo.right\n",
    "            i += 1\n",
    "\n",
    "        pasos.append(f\"Predicción final = {nodo.value:.3f}\")\n",
    "        return \"\\n\".join(pasos)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7988ec4e",
   "metadata": {},
   "source": [
    "#### **2.3. Nos enfrentamos al problema de California Housing**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b137eebc",
   "metadata": {},
   "source": [
    "**1. ¿Cómo evoluciona el error de training y test conforme se aumenta la profundidad del  ́arbol? Base su razonamiento en dos gráficas obtenidas con matplotlib.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ebca09c",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "dd82b034",
   "metadata": {},
   "source": [
    "**2. ¿Sobre qué parámetros tendría sentido realizar cross-validation en nuestro modelo?**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef1f0dbf",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5d98a3a8",
   "metadata": {},
   "source": [
    "**3. Tome un par de muestras y utilizando el método decision_path(x) justifique la predicción dada. Como habrá comprobado, el método decision_path(x) devuelve una lista ordenada de argumentos, suponga que esa lista se desordena: ¿seguiría siendo válida la interpretación de la predicción?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5c579ef",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "20c64217",
   "metadata": {},
   "source": [
    "**4. Para una muestra idéntica a la del apartado anterior, si se entrenara un nuevo árbol, ¿la justificación de su predicción sería la misma?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cae33df2",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "24277e32",
   "metadata": {},
   "source": [
    "**5. Suponga que divide el conjunto de train en dos mitades, entrena dos árboles distintos, uno con cada mitad, y toma una muestra de test. ¿Sería la predicción de la muestra igual en cada árbol? ¿Y la interpretación obtenida con decision_path(x)? ¿Qué repercusión en la interpretación tendría esa situación?**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f407f25",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2b409adc",
   "metadata": {},
   "source": [
    "**6. Utilice la clase DecisionTreeRegressor de Sklearn para resolver de nuevo el problema. Compare el score de ambos modelos y, para alguna muestra, el resultado de decision_path(x) de ambos árboles.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16f64d24",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8cf1ff5b",
   "metadata": {},
   "source": [
    "**7. En base a su implementación ¿cuál es el coste computacional teórico de entrenar un árbol? ¿Y cuál sería el coste en predicción? ¿Y el coste en memoria? Si tuviera que resolver un problema con un conjunto de datos de gran volumen, ¿qué preferiría usar, un árbol o un modelo de regresión lineal?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2810433b",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8e63c150",
   "metadata": {},
   "source": [
    "\n",
    "### **3. Ensembles**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
